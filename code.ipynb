{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import joblib\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score,confusion_matrix\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(y_test: pd.DataFrame, y_pred: np.array):\n",
    "    y_test = y_test.values.flatten()\n",
    "    \n",
    "    class_labels = list(set(y_test))\n",
    "    class_labels.sort()  \n",
    "    accuracy = []\n",
    "\n",
    "    for label in class_labels:\n",
    "        true_positives = sum(1 for yt, yp in zip(y_test, y_pred) if yt == label and yp == label)\n",
    "        total_samples = sum(1 for yt in y_test if yt == label)\n",
    "        \n",
    "        accuracy_temp = true_positives / total_samples if total_samples > 0 else 0.0\n",
    "        accuracy.append(accuracy_temp)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet_name = ['IPB','IAB','OIB','MORB','BABB','OFB','CFB']\n",
    "\n",
    "# columns = ['ID','TIO2(WT%)','P2O5(WT%)','NB(PPM)','TA(PPM)','ZR(PPM)','HF(PPM)','Y(PPM)','LA(PPM)','CE(PPM)','PR(PPM)','ND(PPM)','SM(PPM)','EU(PPM)','GD(PPM)','HO(PPM)','ER(PPM)','YB(PPM)','LU(PPM)','DY(PPM)','TB(PPM)','CR(PPM)','NI(PPM)']\n",
    "columns = ['ID','SIO2(WT%)','TIO2(WT%)','AL2O3(WT%)','FEOT(WT%)','CAO(WT%)','MGO(WT%)','MNO(WT%)','K2O(WT%)','NA2O(WT%)','P2O5(WT%)','RB(PPM)','SR(PPM)','BA(PPM)','TH(PPM)','U(PPM)','NB(PPM)','TA(PPM)','ZR(PPM)','HF(PPM)','Y(PPM)','LA(PPM)','CE(PPM)','PR(PPM)','ND(PPM)','SM(PPM)','EU(PPM)','GD(PPM)','HO(PPM)','ER(PPM)','YB(PPM)','LU(PPM)','DY(PPM)','TB(PPM)','CR(PPM)','NI(PPM)']\n",
    "\n",
    "data_train_path = 'data_train.xlsx'\n",
    "data_test_path = 'data_test.xlsx'\n",
    "\n",
    "data_train = []\n",
    "data_test = []\n",
    "\n",
    "for i in range(len(sheet_name)):\n",
    "    data_train_temp = pd.read_excel(data_train_path, sheet_name=sheet_name[i])\n",
    "    data_train_temp = data_train_temp[columns]\n",
    "    data_train_temp['ID_new'] = i\n",
    "    data_train.append(data_train_temp)\n",
    "               \n",
    "    data_test_temp = pd.read_excel(data_test_path, sheet_name=sheet_name[i])\n",
    "    data_test_temp = data_test_temp[columns]\n",
    "    data_test_temp['ID_new'] = i\n",
    "    data_test.append(data_test_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_train\n",
    "train = pd.concat([data[0], data[1], data[2], data[3], data[4], data[5], data[6]], axis=0)\n",
    "train = train.sample(frac=1, random_state=10)\n",
    "\n",
    "data = data_test\n",
    "test = pd.concat([data[0], data[1], data[2], data[3], data[4], data[5], data[6]], axis=0)\n",
    "test = test.sample(frac=1, random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.iloc[:,1:-1]\n",
    "y_train = train.iloc[:,-1:]\n",
    "\n",
    "X_test = test.iloc[:,1:-1]\n",
    "y_test = test.iloc[:,-1:]\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "accuracy_dict = {}\n",
    "accuracy_dict['category'] = ['IPB','IAB','OIB','MORB','BABB','OFB','CFB','Overall']\n",
    "\n",
    "precision_dict = {}\n",
    "precision_dict['category'] = ['IPB','IAB','OIB','MORB','BABB','OFB','CFB','Overall']\n",
    "\n",
    "recall_dict = {}\n",
    "recall_dict['category'] = ['IPB','IAB','OIB','MORB','BABB','OFB','CFB','Overall']\n",
    "\n",
    "f1_score_dict = {}\n",
    "f1_score_dict['category'] = ['IPB','IAB','OIB','MORB','BABB','OFB','CFB','Overall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# model = svm.SVC()\n",
    "# param = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf'], 'gamma': [0.1, 1, 10]}\n",
    "\n",
    "# model = RandomForestClassifier()\n",
    "# param = {'n_estimators': [100, 200, 300],'criterion': ['gini','entropy','log_loss'],'max_depth': [10, 20, 30]}\n",
    "\n",
    "# model = XGBClassifier()\n",
    "# param = {'booster':['gbtree','dart'], 'n_estimators': [50,100,200,500]}\n",
    "\n",
    "# grid_search = GridSearchCV(estimator=model, param_grid=param, cv=5, scoring='accuracy')\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# best_params = grid_search.best_params_\n",
    "# best_score = grid_search.best_score_\n",
    "# print(\"Best Hyperparameters:\", best_params)\n",
    "# print(\"Best score:\", best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM = svm.SVC(C=10, kernel='rbf')\n",
    "SVM.fit(X_train, y_train)\n",
    "\n",
    "y_pred_SVM = SVM.predict(X_test)\n",
    "\n",
    "accuracy_SVM = accuracy_score(y_test, y_pred_SVM)\n",
    "precision_SVM = precision_score(y_test, y_pred_SVM, average='macro')\n",
    "recall_SVM = recall_score(y_test, y_pred_SVM, average='macro')\n",
    "f1_SVM = f1_score(y_test, y_pred_SVM, average='macro')\n",
    "\n",
    "print('accuracy_SVM = ',accuracy_SVM)\n",
    "print('precision_SVM = ',precision_SVM)\n",
    "print('recall_SVM = ',recall_SVM)\n",
    "print('f1_SVM = ',f1_SVM)\n",
    "print()\n",
    "\n",
    "accuracy = get_accuracy(y_test, y_pred_SVM)\n",
    "precision = precision_score(y_test, y_pred_SVM, average=None, labels=range(len(train['ID'].unique())))\n",
    "recall = recall_score(y_test, y_pred_SVM, average=None, labels=range(len(train['ID'].unique())))\n",
    "f1 = f1_score(y_test, y_pred_SVM, average=None, labels=range(len(train['ID'].unique())))\n",
    "\n",
    "# print('accuracy:',accuracy)\n",
    "# print('precision:',list(precision))\n",
    "# print('recall:',list(recall))\n",
    "# print('f1_score:',list(f1))\n",
    "\n",
    "accuracy_dict['SVM'] = accuracy\n",
    "accuracy_dict['SVM'].append(accuracy_SVM)\n",
    "\n",
    "precision_dict['SVM'] = list(precision)\n",
    "precision_dict['SVM'].append(precision_SVM)\n",
    "\n",
    "recall_dict['SVM'] = list(recall)\n",
    "recall_dict['SVM'].append(recall_SVM)\n",
    "\n",
    "f1_score_dict['SVM'] = list(f1)\n",
    "f1_score_dict['SVM'].append(f1_SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=300, criterion='entropy', max_depth=30)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "precision_rf = precision_score(y_test, y_pred_rf, average='macro')\n",
    "recall_rf = recall_score(y_test, y_pred_rf, average='macro')\n",
    "f1_rf = f1_score(y_test, y_pred_rf, average='macro')\n",
    "\n",
    "print('accuracy_rf = ',accuracy_rf)\n",
    "print('precision_rf = ',precision_rf)\n",
    "print('recall_rf = ',recall_rf)\n",
    "print('f1_rf = ',f1_rf)\n",
    "print()\n",
    "\n",
    "accuracy = get_accuracy(y_test, y_pred_rf)\n",
    "precision = precision_score(y_test, y_pred_rf, average=None, labels=range(len(train['ID'].unique())))\n",
    "recall = recall_score(y_test, y_pred_rf, average=None, labels=range(len(train['ID'].unique())))\n",
    "f1 = f1_score(y_test, y_pred_rf, average=None, labels=range(len(train['ID'].unique())))\n",
    "\n",
    "# print('accuracy:',accuracy)\n",
    "# print('precision:',list(precision))\n",
    "# print('recall:',list(recall))\n",
    "# print('f1_score:',list(f1))\n",
    "\n",
    "accuracy_dict['RF'] = accuracy\n",
    "accuracy_dict['RF'].append(accuracy_rf)\n",
    "\n",
    "precision_dict['RF'] = list(precision)\n",
    "precision_dict['RF'].append(precision_rf)\n",
    "\n",
    "recall_dict['RF'] = list(recall)\n",
    "recall_dict['RF'].append(recall_rf)\n",
    "\n",
    "f1_score_dict['RF'] = list(f1)\n",
    "f1_score_dict['RF'].append(f1_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(booster='gbtree', n_estimators=500)\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "y_pred_xgb = xgb.predict(X_test)\n",
    "\n",
    "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "precision_xgb = precision_score(y_test, y_pred_xgb, average='macro')\n",
    "recall_xgb = recall_score(y_test, y_pred_xgb, average='macro')\n",
    "f1_xgb = f1_score(y_test, y_pred_xgb, average='macro')\n",
    "\n",
    "print('accuracy_xgb = ',accuracy_xgb)\n",
    "print('precision_xgb = ',precision_xgb)\n",
    "print('recall_xgb = ',recall_xgb)\n",
    "print('f1_xgb = ',f1_xgb)\n",
    "print()\n",
    "\n",
    "accuracy = get_accuracy(y_test, y_pred_xgb)\n",
    "precision = precision_score(y_test, y_pred_xgb, average=None, labels=range(len(train['ID'].unique())))\n",
    "recall = recall_score(y_test, y_pred_xgb, average=None, labels=range(len(train['ID'].unique())))\n",
    "f1 = f1_score(y_test, y_pred_xgb, average=None, labels=range(len(train['ID'].unique())))\n",
    "\n",
    "# print('accuracy:',accuracy)\n",
    "# print('precision:',list(precision))\n",
    "# print('recall:',list(recall))\n",
    "# print('f1_score:',list(f1))\n",
    "\n",
    "accuracy_dict['xgb'] = accuracy\n",
    "accuracy_dict['xgb'].append(accuracy_xgb)\n",
    "\n",
    "precision_dict['xgb'] = list(precision)\n",
    "precision_dict['xgb'].append(precision_xgb)\n",
    "\n",
    "recall_dict['xgb'] = list(recall)\n",
    "recall_dict['xgb'].append(recall_xgb)\n",
    "\n",
    "f1_score_dict['xgb'] = list(f1)\n",
    "f1_score_dict['xgb'].append(f1_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_list = [y_pred_SVM, y_pred_rf, y_pred_xgb]\n",
    "confusion_matrix_name = ['SVM Confusion Matrix', 'RandomForest Confusion Matrix', 'XGBoost Confusion Matrix']\n",
    "\n",
    "for i in range(len(y_pred_list)):\n",
    "    confusion_matrix_temp = confusion_matrix(y_test, y_pred_list[i])\n",
    "    cm_percentage = confusion_matrix_temp.astype('float') / confusion_matrix_temp.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    plt.figure(figsize=(7, 5.5))\n",
    "    sns.heatmap(cm_percentage, annot=True, fmt='.2f', cmap='Blues', xticklabels=sheet_name, yticklabels=sheet_name)\n",
    "\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title(confusion_matrix_name[i])\n",
    "#     plt.savefig(f'22elements_{confusion_matrix_name[i]}.png', dpi=500, bbox_inches='tight')\n",
    "    plt.savefig(f'35elements_{confusion_matrix_name[i]}.png', dpi=500, bbox_inches='tight')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joblib.dump(SVM, '22elements_svm.pkl')\n",
    "# joblib.dump(rf, '22elements_rf.pkl')\n",
    "# joblib.dump(xgb, '22elements_xgb.pkl')\n",
    "\n",
    "joblib.dump(SVM, '35elements_svm.pkl')\n",
    "joblib.dump(rf, '35elements_rf.pkl')\n",
    "joblib.dump(xgb, '35elements_xgb.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "explainer = shap.TreeExplainer(xgb, X_train)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# elements_name = ['TiO2','P2O5','Nb','Ta','Zr','Hf','Y','La','Ce','Pr','Nd','Sm','Eu','Gd','Ho','Er','Yb','Lu','Dy','Tb','Cr','Ni']\n",
    "elements_name = ['SiO2','TiO2','Al2O3','FeOt','CaO','MgO','MnO','K2O','Na2O','P2O5','Rb','Sr','Ba','Th','U','Nb','Ta','Zr','Hf','Y','La','Ce','Pr','Nd','Sm','Eu','Gd','Ho','Er','Yb','Lu','Dy','Tb','Cr','Ni']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_display = 22\n",
    "max_display = 35\n",
    "\n",
    "for class_index in range(len(sheet_name)):\n",
    "    shap.plots.violin(shap_values[class_index], features=X_test, feature_names=elements_name, max_display=max_display, plot_type=\"violin\", show=False)\n",
    "    plt.title(f'SHAP Values for {sheet_name[class_index]}')\n",
    "    # plt.savefig(f'22elements_XGBoost SHAP Values for {sheet_name[class_index]}.png', dpi=500, bbox_inches='tight')\n",
    "    plt.savefig(f'35elements_XGBoost SHAP Values for {sheet_name[class_index]}.png', dpi=500, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化所有类别的 SHAP 汇总图\n",
    "shap.summary_plot(shap_values, X_test, feature_names=elements_name, max_display=max_display, class_names=sheet_name, plot_type='bar', show=False)\n",
    "plt.title('XGBoost SHAP Summary')\n",
    "# plt.savefig('22elements_XGBoost_shap_summary_plot.png', dpi=500, bbox_inches='tight')\n",
    "plt.savefig('35elements_XGBoost_shap_summary_plot.png', dpi=500, bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
